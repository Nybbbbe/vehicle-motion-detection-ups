{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ups_utils import train_initial, train_regular, pseudo_labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "training_date = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "#training_date = \"20240408-190635\"\n",
    "BACTH_SIZE = 64\n",
    "MODEL_NAME = f'MDM_IaA_ups_01_dataset_01_labeled_{training_date}'\n",
    "DATASET_FILE_PATH = 'C:/Users/janny/Aalto_project_2/data/full_dataset_IaA.txt'\n",
    "MODEL_DIR = os.path.join(\"C:/Users/janny/Aalto_project_2/models\", MODEL_NAME)\n",
    "TAU_P = 0.90 # Confidence threshold for positive pseudo-labels, default is 0.70\n",
    "TAU_N = 0.1 # Confidence threshold for negative pseudo-labels, default is 0.05\n",
    "KAPPA_P = 0.05 # Uncertainty threshold for positive pseudo-labels, default is 0.05\n",
    "KAPPA_N = 0.005 # Uncertainty threshold for negative pseudo-labels, default is 0.005\n",
    "TEMP_NL = 2.0 # Temperature for generating negative pseudo-labels, default is 2.0\n",
    "P_LABELED = 0.1 # % of data that is pre labeled\n",
    "ITERATIONS = 5\n",
    "MAX_EPOCH = 20\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janny\\AppData\\Local\\Temp\\ipykernel_17708\\1176990817.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  all_data = pd.read_csv(DATASET_FILE_PATH, sep=', ', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "0    13281\n",
      "1     7821\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "destination_dir = os.path.join(MODEL_DIR, 'data')\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "destination_file_path = os.path.join(destination_dir, os.path.basename(DATASET_FILE_PATH))\n",
    "shutil.copy(DATASET_FILE_PATH, destination_file_path)\n",
    "DATASET_FILE_PATH = destination_file_path\n",
    "\n",
    "all_data = pd.read_csv(DATASET_FILE_PATH, sep=', ', header=None)\n",
    "class_counts = all_data.iloc[:, 12].value_counts()\n",
    "print(class_counts)\n",
    "sampled_data = pd.DataFrame()\n",
    "for label, count in class_counts.items():\n",
    "    # Sample 10% of the data for the current class\n",
    "    sampled_df = all_data[all_data.iloc[:, 12] == label].sample(frac=0.1, random_state=SEED)\n",
    "    sampled_data = pd.concat([sampled_data, sampled_df])\n",
    "\n",
    "SAMPLED_DATASET_FILE_PATH = os.path.join(MODEL_DIR, 'data', 'dataset_01.csv')\n",
    "sampled_data.to_csv(SAMPLED_DATASET_FILE_PATH, index=False, header=None)\n",
    "\n",
    "DATASET_FILE_PATH = SAMPLED_DATASET_FILE_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeToWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        # Make sure it's a float for division to work properly in Python 2\n",
    "        original_width = img_tensor.shape[2]\n",
    "        original_height = img_tensor.shape[1]\n",
    "        aspect_ratio = float(original_height) / float(original_width)\n",
    "        target_height = int(self.target_width * aspect_ratio)\n",
    "\n",
    "        # Resize the tensor\n",
    "        img_tensor = F.resize(img_tensor, [target_height, self.target_width])\n",
    "        return img_tensor\n",
    "    \n",
    "class Normalize3Channel:\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        # Check if the image has 3 channels\n",
    "        img_tensor = img_tensor.float() / 255.0\n",
    "        if img_tensor.shape[0] == 3:\n",
    "            img_tensor = F.normalize(img_tensor, mean=self.mean, std=self.std)\n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, annotations, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_paths = row.iloc[:5].values  # Paths to the image files.\n",
    "        spectrogram_paths = row.iloc[5:12].values  # Paths to the audio spectrogram files.\n",
    "        label = row.iloc[12]\n",
    "        \n",
    "        # Load images and spectrograms, apply the same transform if available\n",
    "        images = [read_image(path) for path in img_paths]\n",
    "        spectrograms = [read_image(path) for path in spectrogram_paths]\n",
    "        \n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "            spectrograms = [self.transform(spectrogram) for spectrogram in spectrograms]\n",
    "\n",
    "        return images, spectrograms, label\n",
    "    \n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, annotations, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_paths = row.iloc[:5].values  # Paths to the image files.\n",
    "        spectrogram_paths = row.iloc[5:12].values  # Paths to the audio spectrogram files.\n",
    "        label = row.iloc[12]\n",
    "        \n",
    "        # Load images and spectrograms, apply the same transform if available\n",
    "        images = [read_image(path) for path in img_paths]\n",
    "        spectrograms = [read_image(path) for path in spectrogram_paths]\n",
    "        \n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "            spectrograms = [self.transform(spectrogram) for spectrogram in spectrograms]\n",
    "\n",
    "        return images, spectrograms, label, idx\n",
    "    \n",
    "class PseudoLabeledDataset(Dataset):\n",
    "    def __init__(self, annotations, pseudo_labels, transform=None):\n",
    "        \"\"\"\n",
    "        annotations: DataFrame containing the data paths and original labels.\n",
    "        pseudo_labels: List or array containing the pseudo labels for the data.\n",
    "        negative_label_mask: List or array indicating if a label is a negative pseudo-label.\n",
    "        transform: A function/transform that takes in an PIL image and returns a transformed version.\n",
    "        \"\"\"\n",
    "        self.annotations = annotations\n",
    "        self.pseudo_labels = pseudo_labels\n",
    "        # self.negative_label_mask = negative_label_mask\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_paths = row.iloc[:5].values  # Paths to the image files.\n",
    "        spectrogram_paths = row.iloc[5:12].values  # Paths to the audio spectrogram files.\n",
    "        pseudo_label = self.pseudo_labels[idx]\n",
    "        # is_negative = self.negative_label_mask[idx]\n",
    "        \n",
    "        # Assume `read_image` function is defined elsewhere\n",
    "        images = [read_image(path) for path in img_paths]\n",
    "        spectrograms = [read_image(path) for path in spectrogram_paths]\n",
    "        \n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "            spectrograms = [self.transform(spectrogram) for spectrogram in spectrograms]\n",
    "\n",
    "        return images, spectrograms, pseudo_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatasets(pseudo_labels_dict=None):\n",
    "    transform = transforms.Compose([\n",
    "        ResizeToWidth(512),  # Resize width to 512 pixels while maintaining aspect ratio\n",
    "        Normalize3Channel(),\n",
    "    ])\n",
    "\n",
    "    all_data = pd.read_csv(DATASET_FILE_PATH, sep=',')\n",
    "    train_data, test_data = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "    labeled, unlabeled = train_test_split(train_data, test_size=(1 - P_LABELED), random_state=42)\n",
    "\n",
    "    labeled_dataset = LabeledDataset(labeled, transform)\n",
    "    unlabeled_dataset = UnlabeledDataset(unlabeled, transform)\n",
    "    pseudo_labeled_dataset = unlabeled_dataset\n",
    "    test_dataset = LabeledDataset(test_data, transform)\n",
    "\n",
    "    if pseudo_labels_dict is not None:\n",
    "        psuedo_labeled_indexes = pseudo_labels_dict['psuedo_labeled_indexes']\n",
    "        psuedo_labeled_targets = pseudo_labels_dict['psuedo_labeled_targets']\n",
    "        # negative_label_mask = pseudo_labels_dict['negative_label_mask']\n",
    "\n",
    "        pseudo_labeled_data = unlabeled.iloc[psuedo_labeled_indexes]\n",
    "        pseudo_labeled_dataset = PseudoLabeledDataset(\n",
    "            pseudo_labeled_data, psuedo_labeled_targets, transform\n",
    "        )\n",
    "\n",
    "    return labeled_dataset, pseudo_labeled_dataset, unlabeled_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_features):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-1])  # Remove the last layer\n",
    "\n",
    "        # Freeze the parameters in the feature extraction layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Linear(resnet50.fc.in_features, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class AudioFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_features):\n",
    "        super(AudioFeatureExtractor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 23 * 64, output_features)  # Adjust the size based on your input dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, image_output_features, audio_output_features, num_classes):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.image_extractor = ImageFeatureExtractor(image_output_features)\n",
    "        self.audio_extractor = AudioFeatureExtractor(audio_output_features)\n",
    "\n",
    "        # Combine features from both extractors\n",
    "        total_features = image_output_features * 5 + audio_output_features * 7\n",
    "        self.classifier = nn.Linear(total_features, num_classes)\n",
    "\n",
    "    def forward(self, image_0, image_1, image_2, image_3, image_4,\n",
    "        audio_0, audio_1, audio_2, audio_3, audio_4, audio_5, audio_6\n",
    "    ):\n",
    "        image_features_0 = self.image_extractor(image_0)\n",
    "        image_features_1 = self.image_extractor(image_1)\n",
    "        image_features_2 = self.image_extractor(image_2)\n",
    "        image_features_3 = self.image_extractor(image_3)\n",
    "        image_features_4 = self.image_extractor(image_4)\n",
    "\n",
    "        audio_features_0 = self.audio_extractor(audio_0)\n",
    "        audio_features_1 = self.audio_extractor(audio_1)\n",
    "        audio_features_2 = self.audio_extractor(audio_2)\n",
    "        audio_features_3 = self.audio_extractor(audio_3)\n",
    "        audio_features_4 = self.audio_extractor(audio_4)\n",
    "        audio_features_5 = self.audio_extractor(audio_5)\n",
    "        audio_features_6 = self.audio_extractor(audio_6)\n",
    "        \n",
    "        combined_features = torch.cat((\n",
    "            image_features_0, image_features_1, image_features_2, image_features_3, image_features_4,\n",
    "            audio_features_0, audio_features_1, audio_features_2, audio_features_3, audio_features_4,\n",
    "            audio_features_5, audio_features_6\n",
    "        ), dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:14<00:00,  4.74s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.74s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.80s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.59s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.70s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.52s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.72s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.52s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.68s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.57s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.72s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.47s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.65s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.53s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.64s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.73s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.66s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.68s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.63s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.69s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.50s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.44s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.51s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.68s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.59s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.58s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.73s/it]\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.69s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.77s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.57s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.70s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.51s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.69s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.53s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.71s/it]\n",
      "100%|██████████| 27/27 [01:56<00:00,  4.31s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.55s/it]\n",
      "100%|██████████| 14/14 [01:02<00:00,  4.48s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.62s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.47s/it]\n",
      "100%|██████████| 14/14 [01:03<00:00,  4.52s/it]\n",
      "100%|██████████| 4/4 [00:15<00:00,  3.78s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n",
      "100%|██████████| 14/14 [01:01<00:00,  4.37s/it]\n",
      "100%|██████████| 4/4 [00:16<00:00,  4.09s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.53s/it]\n",
      "100%|██████████| 14/14 [00:59<00:00,  4.27s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.40s/it]\n",
      "100%|██████████| 14/14 [01:00<00:00,  4.32s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.34s/it]\n",
      "100%|██████████| 14/14 [01:00<00:00,  4.33s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 27/27 [01:55<00:00,  4.26s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.43s/it]\n",
      "100%|██████████| 20/20 [01:26<00:00,  4.32s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.54s/it]\n",
      "100%|██████████| 20/20 [01:26<00:00,  4.31s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.37s/it]\n",
      "100%|██████████| 20/20 [01:26<00:00,  4.32s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.33s/it]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.30s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.55s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.27s/it]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.29s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.55s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.32s/it]\n",
      "100%|██████████| 20/20 [01:26<00:00,  4.33s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.61s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.33s/it]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.28s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "100%|██████████| 27/27 [01:54<00:00,  4.25s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.45s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.38s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.46s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.39s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.58s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.37s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.38s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.63s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.40s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.38s/it]\n",
      "100%|██████████| 22/22 [01:37<00:00,  4.42s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.56s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.42s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.39s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.63s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.38s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.41s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.50s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.34s/it]\n",
      "100%|██████████| 22/22 [01:36<00:00,  4.38s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.61s/it]\n",
      "100%|██████████| 27/27 [01:55<00:00,  4.27s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.45s/it]\n",
      "100%|██████████| 23/23 [01:39<00:00,  4.32s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.46s/it]\n",
      "100%|██████████| 23/23 [01:39<00:00,  4.33s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.46s/it]\n",
      "100%|██████████| 23/23 [01:40<00:00,  4.36s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.40s/it]\n",
      "100%|██████████| 23/23 [01:39<00:00,  4.34s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.37s/it]\n",
      "100%|██████████| 23/23 [01:39<00:00,  4.34s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.40s/it]\n",
      "100%|██████████| 23/23 [01:39<00:00,  4.33s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.57s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.39s/it]\n",
      "100%|██████████| 23/23 [01:44<00:00,  4.53s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.67s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.54s/it]\n",
      "100%|██████████| 23/23 [01:43<00:00,  4.50s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.64s/it]\n",
      "100%|██████████| 23/23 [01:43<00:00,  4.51s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.66s/it]\n",
      "100%|██████████| 27/27 [01:59<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_ups():\n",
    "    pseudo_labels_dict = None\n",
    "    # pseudo_labels_dict = {\n",
    "    #     \"psuedo_labeled_indexes\": [0, 2, 4],\n",
    "    #     \"psuedo_labeled_targets\": [0, 0, 1],\n",
    "    #     \"negative_label_mask\": [1, 0, 1]\n",
    "    # }\n",
    "\n",
    "    for itr in range(ITERATIONS):\n",
    "        # get labeled_dataset, negative_label_dataset, unlabeled_dataset, test_dataset\n",
    "        labeled_dataset, pseudo_labeled_dataset, unlabeled_dataset, test_dataset = getDatasets(pseudo_labels_dict)\n",
    "\n",
    "        labeled_dataloader = DataLoader(labeled_dataset, batch_size=BACTH_SIZE, shuffle=False)\n",
    "        pseudo_labeled_dataloader = DataLoader(pseudo_labeled_dataset, batch_size=BACTH_SIZE, shuffle=False)\n",
    "        unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=BACTH_SIZE, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=BACTH_SIZE, shuffle=False)\n",
    "\n",
    "        # Initialize new model\n",
    "        image_output_features = 512 # Adjust as needed\n",
    "        audio_output_features = 256  # Adjust as needed\n",
    "        num_classes = 2  # Adjust based on your dataset\n",
    "\n",
    "        model = CombinedModel(image_output_features, audio_output_features, num_classes)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Learning rate can be adjusted\n",
    "        save_dir = f'{MODEL_DIR}/itr_{itr}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        if itr == 0:\n",
    "            train_initial(MAX_EPOCH, model, labeled_dataloader, test_dataloader, optimizer, device, save_dir)\n",
    "        else:\n",
    "            train_regular(MAX_EPOCH, model, labeled_dataloader, pseudo_labeled_dataloader, test_dataloader, optimizer, device, save_dir)\n",
    "\n",
    "        model = torch.load(f'{save_dir}/best_model.pth').to(device)\n",
    "\n",
    "        pseudo_labels_dict = pseudo_labeling(unlabeled_dataloader, model, device, TEMP_NL, KAPPA_N, TAU_N,\n",
    "                        TAU_P, KAPPA_P, no_uncertainty=True)\n",
    "        \n",
    "\n",
    "try:\n",
    "    with open(f'{MODEL_DIR}/output.txt', 'w') as f:\n",
    "        # Saving the original stdout \n",
    "        original_stdout = sys.stdout \n",
    "        sys.stdout = f  # Change the standard output to the file we created.\n",
    "        \n",
    "        run_ups()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")  # This prints the error to the console.\n",
    "    \n",
    "finally:\n",
    "    sys.stdout = original_stdout  # Reset the standard output to its original value,\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
